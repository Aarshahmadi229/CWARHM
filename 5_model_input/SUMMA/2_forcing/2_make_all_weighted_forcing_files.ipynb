{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create all area-weighted forcing files\n",
    "We need to find how the ERA5 gridded forcing maps onto the catchment to create area-weighted forcing as SUMMA input. This involves two steps:\n",
    "1. Intersect the ERA5 shape with the user's catchment shape to find the overlap between a given (sub) catchment and the forcing grid;\n",
    "2. Create an area-weighted, catchment-averaged forcing time series.\n",
    "\n",
    "The CANDEX package (https://github.com/ShervanGharari/candex_newgen) provides the necessary functionality to do this. CANDEX performs the GIS step (1, shapefile intersection) and the area-weighting step (2, create new forcing `.nc` files) as part of a single `run_candex()` call. To allow for parallelization, CANDEX can save the output from the GIS step into a restart `.csv` file which can be used to skip the GIS step. This allows (manual) parallelization of area-weighted forcing file generation after the GIS procedures have been run once. The full workflow is thus:\n",
    "1. [Previous script] Call `run_candex()` with ERA5 and user's shapefile, and one ERA5 forcing `.nc` file;\n",
    "    - CANDEX performs intersection of both shapefiles;\n",
    "    - CANDEX saves the outcomes of this intersection to a `.csv` file;\n",
    "    - CANDEX creates an area-weighted forcing file from a single provided ERA5 source `.nc` file\n",
    "2. [This script] Call `run_candex()` with intersection `.csv` file and all other forcing `.nc` files.\n",
    "3. [Follow-up script] Apply lapse rates to temperature variable.\n",
    "\n",
    "Parallelization of step 2 (2nd `run_candex()` call) requires an external loop that sends (batches of) the remaining ERA5 raw forcing files to individual processors. As with other steps that may be parallelized, creating code that does this is left to the user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import os\n",
    "import candex\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "from shutil import copyfile\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control file handling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy access to control file folder\n",
    "controlFolder = Path('../../../0_control_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the name of the 'active' file in a variable\n",
    "controlFile = 'control_active.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract a given setting from the control file\n",
    "def read_from_control( file, setting ):\n",
    "    \n",
    "    # Open 'control_active.txt' and ...\n",
    "    with open(file) as contents:\n",
    "        for line in contents:\n",
    "            \n",
    "            # ... find the line with the requested setting\n",
    "            if setting in line:\n",
    "                break\n",
    "    \n",
    "    # Extract the setting's value\n",
    "    substring = line.split('|',1)[1]      # Remove the setting's name (split into 2 based on '|', keep only 2nd part)\n",
    "    substring = substring.split('#',1)[0] # Remove comments, does nothing if no '#' is found\n",
    "    substring = substring.strip()         # Remove leading and trailing whitespace, tabs, newlines\n",
    "       \n",
    "    # Return this value    \n",
    "    return substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to specify a default path\n",
    "def make_default_path(suffix):\n",
    "    \n",
    "    # Get the root path\n",
    "    rootPath = Path( read_from_control(controlFolder/controlFile,'root_path') )\n",
    "    \n",
    "    # Get the domain folder\n",
    "    domainName = read_from_control(controlFolder/controlFile,'domain_name')\n",
    "    domainFolder = 'domain_' + domainName\n",
    "    \n",
    "    # Specify the forcing path\n",
    "    defaultPath = rootPath / domainFolder / suffix\n",
    "    \n",
    "    return defaultPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find where the CANDEX restart file is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersected shapefile path. Name is set by CANDEX as [prefix]_intersected_shapefile.shp\n",
    "intersect_path = read_from_control(controlFolder/controlFile,'intersect_forcing_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify default path if needed\n",
    "if intersect_path == 'default':\n",
    "    intersect_path = make_default_path('shapefiles/catchment_intersection/with_forcing') # outputs a Path()\n",
    "else:\n",
    "    intersect_path = Path(intersect_path) # make sure a user-specified path is a Path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remapping filename\n",
    "domain = read_from_control(controlFolder/controlFile,'domain_name')\n",
    "remap_file = domain + '_remapping.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the forcing files (merged ERA5 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of merged ERA5 files\n",
    "forcing_merged_path = read_from_control(controlFolder/controlFile,'forcing_merged_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify default path if needed\n",
    "if forcing_merged_path == 'default':\n",
    "    forcing_merged_path = make_default_path('forcing/2_merged_data') # outputs a Path()\n",
    "else:\n",
    "    forcing_merged_path = Path(forcing_merged_path) # make sure a user-specified path is a Path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find files in folder\n",
    "forcing_files = [forcing_merged_path/file for file in os.listdir(forcing_merged_path) if os.path.isfile(forcing_merged_path/file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the files\n",
    "forcing_files.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find where the area-weighted forcing needs to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location for SUMMA-ready files\n",
    "forcing_basin_path = read_from_control(controlFolder/controlFile,'forcing_basin_avg_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify default path if needed\n",
    "if forcing_basin_path == 'default':\n",
    "    forcing_basin_path = make_default_path('forcing/3_basin_averaged_data') # outputs a Path()\n",
    "else:\n",
    "    forcing_basin_path = Path(forcing_basin_path) # make sure a user-specified path is a Path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the folder if it doesn't exist\n",
    "forcing_basin_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CANDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a CANDEX object\n",
    "cndx = candex.candex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author name\n",
    "cndx.author_name = 'SUMMA public workflow scripts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data license\n",
    "cndx.license = 'Copernicus data use license: https://cds.climate.copernicus.eu/api/v2/terms/static/licence-to-use-copernicus-products.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case name, used in CANDEX-generated file naes\n",
    "cndx.case_name = read_from_control(controlFolder/controlFile,'domain_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERA5 netcdf variable names\n",
    "cndx.var_names = ['airpres',\n",
    "                  'LWRadAtm',\n",
    "                  'SWRadAtm',\n",
    "                  'pptrate',\n",
    "                  'airtemp',\n",
    "                  'spechum',\n",
    "                  'windspd'] # variable names of forcing data - hardcoded because we prescribe them during ERA5 merging\n",
    "cndx.var_lat   = 'latitude'  # name of the latitude dimensions\n",
    "cndx.var_lon   = 'longitude' # name of the longitude dimension\n",
    "cndx.var_time  = 'time'      # name of the time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary folder where the CANDEX-generated GIS files and remapping file will be saved\n",
    "cndx.temp_dir = '' # Force this to be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output folder where the catchment-averaged forcing will be saved\n",
    "cndx.output_dir = str(forcing_basin_path) + '/' # Path() to string; ensure the trailing '/' CANDEX wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netcdf settings\n",
    "cndx.remapped_dim_id = 'hru'     # name of the non-time dimension; prescribed by SUMMA\n",
    "cndx.remapped_var_id = 'hruId'   # name of the variable associated with the non-time dimension\n",
    "cndx.format_list     = ['f4']    # variable type to save forcing as. Single entry here will be used for all variables\n",
    "cndx.fill_value_list = ['-9999'] # fill value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag that we do not want the data stored in .csv in addition to .nc\n",
    "cndx.save_csv  = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag that we currently have no remapping file\n",
    "cndx.remap_csv = str(intersect_path / remap_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce that we want our HRUs returned in the order we put them in\n",
    "cndx.sort_ID = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run candex - this can should be parallelized for speed ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No temporary folder is provided for candex; this will result in candex saving the files in the same directory as python script\n",
      "candex is given multiple varibales to be remapped but only on format and fill valuecandex repeat the format and fill value for all the variables in output files\n",
      "remap file is provided; candex will use this file and skip calculation of remapping\n",
      "candex case exists in the remap file\n",
      "candex detects that the varibales from the netCDF files are identicalin dimensions of the varibales and latitude and longitude\n",
      "candex detects that all the varibales have dimensions of:\n",
      "['time', 'latitude', 'longitude']\n",
      "candex detects that the longitude varibales has dimensions of:\n",
      "['longitude']\n",
      "candex detects that the latitude varibales has dimensions of:\n",
      "['latitude']\n",
      "------REMAPPING------\n",
      "Remapping C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\2_merged_data\\ERA5_merged_197902.nc to C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\3_basin_averaged_data/BowAtBanff_remapped_1979-02-01-00-00-00.nc\n",
      "Started at date and time 2021-04-08 07:56:54.709602\n",
      "Ended   at date and time 2021-04-08 07:58:26.207717\n",
      "------\n",
      "No temporary folder is provided for candex; this will result in candex saving the files in the same directory as python script\n",
      "remap file is provided; candex will use this file and skip calculation of remapping\n",
      "candex case exists in the remap file\n",
      "candex detects that the varibales from the netCDF files are identicalin dimensions of the varibales and latitude and longitude\n",
      "candex detects that all the varibales have dimensions of:\n",
      "['time', 'latitude', 'longitude']\n",
      "candex detects that the longitude varibales has dimensions of:\n",
      "['longitude']\n",
      "candex detects that the latitude varibales has dimensions of:\n",
      "['latitude']\n",
      "------REMAPPING------\n",
      "Remapping C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\2_merged_data\\ERA5_merged_197903.nc to C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\3_basin_averaged_data/BowAtBanff_remapped_1979-03-01-00-00-00.nc\n",
      "Started at date and time 2021-04-08 07:58:26.617846\n",
      "Ended   at date and time 2021-04-08 07:59:33.147662\n",
      "------\n",
      "No temporary folder is provided for candex; this will result in candex saving the files in the same directory as python script\n",
      "remap file is provided; candex will use this file and skip calculation of remapping\n",
      "candex case exists in the remap file\n",
      "candex detects that the varibales from the netCDF files are identicalin dimensions of the varibales and latitude and longitude\n",
      "candex detects that all the varibales have dimensions of:\n",
      "['time', 'latitude', 'longitude']\n",
      "candex detects that the longitude varibales has dimensions of:\n",
      "['longitude']\n",
      "candex detects that the latitude varibales has dimensions of:\n",
      "['latitude']\n",
      "------REMAPPING------\n",
      "Remapping C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\2_merged_data\\ERA5_merged_197904.nc to C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\3_basin_averaged_data/BowAtBanff_remapped_1979-04-01-00-00-00.nc\n",
      "Started at date and time 2021-04-08 07:59:33.237792\n",
      "Ended   at date and time 2021-04-08 08:00:33.778170\n",
      "------\n",
      "No temporary folder is provided for candex; this will result in candex saving the files in the same directory as python script\n",
      "remap file is provided; candex will use this file and skip calculation of remapping\n",
      "candex case exists in the remap file\n",
      "candex detects that the varibales from the netCDF files are identicalin dimensions of the varibales and latitude and longitude\n",
      "candex detects that all the varibales have dimensions of:\n",
      "['time', 'latitude', 'longitude']\n",
      "candex detects that the longitude varibales has dimensions of:\n",
      "['longitude']\n",
      "candex detects that the latitude varibales has dimensions of:\n",
      "['latitude']\n",
      "------REMAPPING------\n",
      "Remapping C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\2_merged_data\\ERA5_merged_197905.nc to C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\3_basin_averaged_data/BowAtBanff_remapped_1979-05-01-00-00-00.nc\n",
      "Started at date and time 2021-04-08 08:00:33.858100\n",
      "Ended   at date and time 2021-04-08 08:01:46.098078\n",
      "------\n",
      "No temporary folder is provided for candex; this will result in candex saving the files in the same directory as python script\n",
      "remap file is provided; candex will use this file and skip calculation of remapping\n",
      "candex case exists in the remap file\n",
      "candex detects that the varibales from the netCDF files are identicalin dimensions of the varibales and latitude and longitude\n",
      "candex detects that all the varibales have dimensions of:\n",
      "['time', 'latitude', 'longitude']\n",
      "candex detects that the longitude varibales has dimensions of:\n",
      "['longitude']\n",
      "candex detects that the latitude varibales has dimensions of:\n",
      "['latitude']\n",
      "------REMAPPING------\n",
      "Remapping C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\2_merged_data\\ERA5_merged_197906.nc to C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\3_basin_averaged_data/BowAtBanff_remapped_1979-06-01-00-00-00.nc\n",
      "Started at date and time 2021-04-08 08:01:46.287925\n",
      "Ended   at date and time 2021-04-08 08:03:08.857951\n",
      "------\n",
      "No temporary folder is provided for candex; this will result in candex saving the files in the same directory as python script\n",
      "remap file is provided; candex will use this file and skip calculation of remapping\n",
      "candex case exists in the remap file\n",
      "candex detects that the varibales from the netCDF files are identicalin dimensions of the varibales and latitude and longitude\n",
      "candex detects that all the varibales have dimensions of:\n",
      "['time', 'latitude', 'longitude']\n",
      "candex detects that the longitude varibales has dimensions of:\n",
      "['longitude']\n",
      "candex detects that the latitude varibales has dimensions of:\n",
      "['latitude']\n",
      "------REMAPPING------\n",
      "Remapping C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\2_merged_data\\ERA5_merged_197907.nc to C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\3_basin_averaged_data/BowAtBanff_remapped_1979-07-01-00-00-00.nc\n",
      "Started at date and time 2021-04-08 08:03:08.968047\n",
      "Ended   at date and time 2021-04-08 08:04:10.918041\n",
      "------\n",
      "No temporary folder is provided for candex; this will result in candex saving the files in the same directory as python script\n",
      "remap file is provided; candex will use this file and skip calculation of remapping\n",
      "candex case exists in the remap file\n",
      "candex detects that the varibales from the netCDF files are identicalin dimensions of the varibales and latitude and longitude\n",
      "candex detects that all the varibales have dimensions of:\n",
      "['time', 'latitude', 'longitude']\n",
      "candex detects that the longitude varibales has dimensions of:\n",
      "['longitude']\n",
      "candex detects that the latitude varibales has dimensions of:\n",
      "['latitude']\n",
      "------REMAPPING------\n",
      "Remapping C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\2_merged_data\\ERA5_merged_197908.nc to C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\3_basin_averaged_data/BowAtBanff_remapped_1979-08-01-00-00-00.nc\n",
      "Started at date and time 2021-04-08 08:04:11.008410\n",
      "Ended   at date and time 2021-04-08 08:05:17.388238\n",
      "------\n",
      "No temporary folder is provided for candex; this will result in candex saving the files in the same directory as python script\n",
      "remap file is provided; candex will use this file and skip calculation of remapping\n",
      "candex case exists in the remap file\n",
      "candex detects that the varibales from the netCDF files are identicalin dimensions of the varibales and latitude and longitude\n",
      "candex detects that all the varibales have dimensions of:\n",
      "['time', 'latitude', 'longitude']\n",
      "candex detects that the longitude varibales has dimensions of:\n",
      "['longitude']\n",
      "candex detects that the latitude varibales has dimensions of:\n",
      "['latitude']\n",
      "------REMAPPING------\n",
      "Remapping C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\2_merged_data\\ERA5_merged_197909.nc to C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\3_basin_averaged_data/BowAtBanff_remapped_1979-09-01-00-00-00.nc\n",
      "Started at date and time 2021-04-08 08:05:17.488062\n",
      "Ended   at date and time 2021-04-08 08:06:20.733421\n",
      "------\n",
      "No temporary folder is provided for candex; this will result in candex saving the files in the same directory as python script\n",
      "remap file is provided; candex will use this file and skip calculation of remapping\n",
      "candex case exists in the remap file\n",
      "candex detects that the varibales from the netCDF files are identicalin dimensions of the varibales and latitude and longitude\n",
      "candex detects that all the varibales have dimensions of:\n",
      "['time', 'latitude', 'longitude']\n",
      "candex detects that the longitude varibales has dimensions of:\n",
      "['longitude']\n",
      "candex detects that the latitude varibales has dimensions of:\n",
      "['latitude']\n",
      "------REMAPPING------\n",
      "Remapping C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\2_merged_data\\ERA5_merged_197910.nc to C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\3_basin_averaged_data/BowAtBanff_remapped_1979-10-01-00-00-00.nc\n",
      "Started at date and time 2021-04-08 08:06:20.828044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended   at date and time 2021-04-08 08:07:42.828303\n",
      "------\n",
      "No temporary folder is provided for candex; this will result in candex saving the files in the same directory as python script\n",
      "remap file is provided; candex will use this file and skip calculation of remapping\n",
      "candex case exists in the remap file\n",
      "candex detects that the varibales from the netCDF files are identicalin dimensions of the varibales and latitude and longitude\n",
      "candex detects that all the varibales have dimensions of:\n",
      "['time', 'latitude', 'longitude']\n",
      "candex detects that the longitude varibales has dimensions of:\n",
      "['longitude']\n",
      "candex detects that the latitude varibales has dimensions of:\n",
      "['latitude']\n",
      "------REMAPPING------\n",
      "Remapping C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\2_merged_data\\ERA5_merged_197911.nc to C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\3_basin_averaged_data/BowAtBanff_remapped_1979-11-01-00-00-00.nc\n",
      "Started at date and time 2021-04-08 08:07:42.938045\n",
      "Ended   at date and time 2021-04-08 08:09:12.729600\n",
      "------\n",
      "No temporary folder is provided for candex; this will result in candex saving the files in the same directory as python script\n",
      "remap file is provided; candex will use this file and skip calculation of remapping\n",
      "candex case exists in the remap file\n",
      "candex detects that the varibales from the netCDF files are identicalin dimensions of the varibales and latitude and longitude\n",
      "candex detects that all the varibales have dimensions of:\n",
      "['time', 'latitude', 'longitude']\n",
      "candex detects that the longitude varibales has dimensions of:\n",
      "['longitude']\n",
      "candex detects that the latitude varibales has dimensions of:\n",
      "['latitude']\n",
      "------REMAPPING------\n",
      "Remapping C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\2_merged_data\\ERA5_merged_197912.nc to C:\\Globus endpoint\\summaWorkflow_data\\domain_BowAtBanff\\forcing\\3_basin_averaged_data/BowAtBanff_remapped_1979-12-01-00-00-00.nc\n",
      "Started at date and time 2021-04-08 08:09:12.887760\n",
      "Ended   at date and time 2021-04-08 08:10:47.696423\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Loop over the remaining forcing files\n",
    "for file in forcing_files[1:]: # skip the first one, as we completed that in the previous script\n",
    "    \n",
    "    # ERA5 forcing files to use\n",
    "    cndx.source_nc = str(file) # Path() to string\n",
    "    \n",
    "    # Note on centroid warnings: in this case we use a regular lat/lon grid to represent ERA5 forcing and ...\n",
    "    #     centroid estimates without reprojecting are therefore acceptable.\n",
    "    # Note on deprecation warnings: this is a CANDEX issue that cannot be resolved here. Does not affect current use.\n",
    "    cndx.run_candex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code provenance\n",
    "Generates a basic log file in the domain folder and copies the control file and itself there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the log path and file name\n",
    "logPath = forcing_basin_path\n",
    "log_suffix = '_create_all_weighted_forcing_file_log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a log folder\n",
    "logFolder = '_workflow_log'\n",
    "Path( logPath / logFolder ).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy this script\n",
    "thisFile = '2_make_all_weighted_forcing_files.ipynb'\n",
    "copyfile(thisFile, logPath / logFolder / thisFile);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current date and time\n",
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a log file \n",
    "logFile = now.strftime('%Y%m%d') + log_suffix\n",
    "with open( logPath / logFolder / logFile, 'w') as file:\n",
    "    \n",
    "    lines = ['Log generated by ' + thisFile + ' on ' + now.strftime('%Y/%m/%d %H:%M:%S') + '\\n',\n",
    "             'Made all remaining weighted forcing files based on restart file from intersected shapefiles of catchment and ERA5.']\n",
    "    for txt in lines:\n",
    "        file.write(txt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geospatialTools_qgis_candex]",
   "language": "python",
   "name": "conda-env-geospatialTools_qgis_candex-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
